###### VARIABLES YOU SHOULD CONFIGURE FOR YOUR DEPLOYEMNT
###### OR PASS as "-e" args to ansible-playbook command

# This is an account that must exist in OpenStack.
# It is used to create projects, access, Heat templates
admin_user: opentlc-mgr

# Authenication credentials for OpenStack in order to create the things.
# These should be included with your secrets, but are listed here for reference
# osp_auth_url:
# osp_auth_username:
# osp_auth_password:
# osp_auth_cloud:
# osp_auth_project_domain: #usually set to "default"
# osp_auth_user_domain: #usually set to "default"

# The output_dir holds all of the files generated during the deployment
# This includes generated Heat templates, SSH config, SSH keys
# This must be an absolute path and no vars (like $HOME or ~)
output_dir: /tmp/output_dir

# The name of the agnosticd config to deploy
env_type: rhv-lab

# The {{ guid }} is used everywhere and it is what differentiates otherwise
# identical environments. Make this unique. Usually they are 4 characters, but
# it can be any reasonablre length.
guid: mydefault

# The name of the project that will be created in OpenStack for the user
osp_project_name: "{{ guid }}-project"

# Set this to true if you need to create a new project in OpenStack
# This should almost always be set to true for RHV installations
# If it is set to false, the {{ osp_project_name }} must already exist and
# should be able to run whatever you are deploying
osp_project_create: true

# The name of the RHV cluster that will be deployed.
# This is primarily used if you want to automate the RHV deployment.
cluster_name: cluster-{{ guid }}

# Used to add metadata (tags) to OpenStack objects created
project_tag: "{{ env_type }}-{{ guid }}"

# Why is this config being deployed?
# Some valid: development, ilt, production, event
purpose: development

# The type of cloud provider this will be deployed to
cloud_provider: osp

# This should be overwritten based on the user ordering the catalog item
# It will be used by the bastion-student-user role and created on the bastion
student_name: lab-user

# Enable this if you want to create a user on the bastion
# Mutually exclusive with {{ install_ipa_client }}
install_student_user: true

# Enable this if you want to use IPA for user authentication.
# Mutually exclusive with {{ install_student_user }}
install_ipa_client: false

# TODO: What does this really do besides run the role?
set_env_authorized_key: true
env_authorized_key: "{{guid}}key"
key_name: "default_key_name"

# This is the user that Ansible will use to connect to the nodes it is
# configuring from the admin/control host
ansible_user: cloud-user
remote_user: cloud-user

# Run the bastion-lite role
install_bastion: true

# TODO: Decide on whether to use sat or give access to repos directly with key
# This will tell Agnosticd to use either:
# sattelite, rhn, or file for repos
repo_method: file
# If using satellite, these are needed:
# satellite_url: satellite.opentlc.com
# satellite_activationkey: # This should be stored in secrets
# satellite_org: # This should be stored in secrets
# use_content_view: true
# If using file, these are needed in addition to the repos_template.j2 file:
osprelease: 13
repo_version: '{{ osprelease }}'
own_repo_path: # Should be defined in secrets

# Packages to install on all of the hosts deployed as part of the agnosticd config
# This invokes the "common" role
install_common: true

# As part of the "common" role, this cause it to do a yum update on the host
update_packages: true

# The packages that will be installed by the "common" role. Only put things
# in this list that are needed, stable, and useful on every node.
common_packages:
  - unzip
  - bash-completion
  - tmux
  - bind-utils
  - wget
  - ansible
  - git
  - vim-enhanced
  - httpd-tools
  - openldap-clients
  - podman
  - tree
  - mc
  - screen
  - strace
  - mtr
#  - atop
#  - htop

# This will run in the post_software phase and run playbooks in the
# software_playbooks directory
software_to_deploy: none

# If you want DNS entries to be created automatically, choose one of these.
# Alternately, they can both be set to false.
use_dynamic_dns: true
# This is not fully implemented yet
# use_route53: false

# The domain that you want to add DNS entries to
rhv_cluster_dns_zone: blue.osp.opentlc.com

# The dynamic DNS server you will add entries to.
# NOTE: This is only applicable when {{ use_dynamic_dns}} is true
rhv_cluster_dns_server: ddns01.opentlc.com

# Whether to wait for an ack from the DNS servers before continuing
wait_for_dns: true

# Authenticaion for DDNS
# ddns_key_name:
# ddns_secret_name:

# Set this to true if you want a FIPs provisioned for an RHV on OpenStack install
# This will provision an API and Ingress FIP
rhv_fip_provision: True

# This requires DDNS or other DNS solution configured
# If enabled, it will add DNS entries for the API and Ingress FIPs
rhv_fip_dns: True

# Quotas to set for new project that is created
# 5 x 32 GB with 8 cores and 50+250 GB
quota_num_instances: 5
quota_num_cores: 40
quota_memory: 163840 # in MB
quota_num_volumes: 10
quota_volumes_gigs: 1500
#quota_loadbalancers: #when Octavia is available
#quota_pool: #when Octavia is available
quota_networks: 4
quota_subnets: 4
quota_routers: 4
quota_fip: 5
quota_sg: 20
quota_sg_rules: 200

# The external network in OpenStack where the floating IPs (FIPs) come from
provider_network: external

# If you are deploying RHV, this should be set to the network that you
# want to use and will be used to create security groups.
# It will pull the subnet CIDR from the defined network below, based on the
# name you define for {{ rhv_network }}
rhv_network: "rhv"
rhv_network_subnet_cidr: "{{ networks | json_query(query_subnet_cidr) | first }}"
query_subnet_cidr: "[?name=='{{ rhv_network }}'].subnet_cidr"

# A list of the private networks and subnets to create in the project
# You can create as many as you want, but at least one is required.
# Use the name of the networks where appropriate in the instance list
networks:
  - name: rhv
    shared: "false"
    subnet_cidr: 192.168.47.0/24
    gateway_ip: 192.168.47.1
    allocation_start: 192.168.47.10
    allocation_end: 192.168.47.254
    dns_nameservers: []
    create_router: true
# Another example network if you need more than one deployed
  - name: gluster
    shared: "false"
    subnet_cidr: 192.48.0.0/24
    gateway_ip: 192.48.0.1
    allocation_start: 192.48.0.25
    allocation_end: 192.48.0.156
    dns_nameservers: []
    create_router: true

# These will influence the rhvh if it is being deployed
rhvh_instance_type: rhv-hypervisor
rhvh_instance_image: rhel-guest-7.7u2

# Instances to be provisioned in new project
# Provide these as a list.
# Each instance type can have any number of replicas deployed with the same
# configuration.
# Metadata in OpenStack is equivelent to tags in AWS
# These instances will be created with Cinder persistent volumes
instances:
  - name: rhvh
    count: 3
    unique: yes
    alt_name: rhv-hypervisor
    image_id: "{{ rhvh_instance_image }}"
    floating_ip: yes
    flavor:
      osp: "{{ rhvh_instance_type }}"
    metadata:
      - AnsibleGroup: "rhvh"
      - function: rhvh
      - user: jo
      - project: "{{ project_tag }}"
      - ostype: linux
      - Purpose: "{{ purpose }}"
    rootfs_size: 50
    networks:
      - rhv
      - gluster
    security_groups:
      - rhvh_sg

# Security groups and associated rules. This will be provided
# when the Heat template is generated separate groups and rules
security_groups:
  - name: rhvh_sg
    description: Security group for RHV hypervisor and bootstrap
    rules:
    - protocol: icmp
      direction: ingress
      description: "icmp"
    - protocol: tcp
      direction: ingress
      port_range_min: 1
      port_range_max: 65535
      remote_ip_prefix: "0.0.0.0/0"
      description: "open it up"
